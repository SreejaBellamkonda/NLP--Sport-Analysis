{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "choice-attraction",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "papermill": {
          "duration": 0.081937,
          "end_time": "2021-04-05T01:54:25.598579",
          "exception": false,
          "start_time": "2021-04-05T01:54:25.516642",
          "status": "completed"
        },
        "tags": [],
        "id": "choice-attraction"
      },
      "source": [
        "# Football Commentary Summarization\n",
        "\n",
        "**There 3 different training models used here**\n",
        "- `build_seq2seq_model_with_just_lstm` - **Seq2Seq model with just LSTMs**. Both `encoder` and `decoder` have just `LSTM`s.\n",
        "- `build_seq2seq_model_with_bidirectional_lstm` - **Seq2Seq model with Bidirectional LSTMs**. Both `encoder` and `decoder` have `Bidirectional LSTM`s.\n",
        "- `build_hybrid_seq2seq_model` - **Seq2Seq model with hybrid architecture**. Here `encoder` has `Bidirectional LSTM`s while `decoder` has just `LSTM`s.\n",
        "\n",
        "**To see the full learning and results of all the 3 model go to the end of the notebook in the `Running all the 3 different models` section**\n",
        "\n",
        "The `model (the trained model)`, `encoder_model (for inference)` and `decoder_model (for inference)` for **Seq2Seq with just LSTMs** are only saved.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cMjBsCPGx24d"
      },
      "id": "cMjBsCPGx24d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rapid-correction",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:54:25.761850Z",
          "iopub.status.busy": "2021-04-05T01:54:25.760634Z",
          "iopub.status.idle": "2021-04-05T01:54:33.451664Z",
          "shell.execute_reply": "2021-04-05T01:54:33.450357Z"
        },
        "papermill": {
          "duration": 7.77362,
          "end_time": "2021-04-05T01:54:33.451872",
          "exception": false,
          "start_time": "2021-04-05T01:54:25.678252",
          "status": "completed"
        },
        "tags": [],
        "id": "rapid-correction"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import string\n",
        "import unicodedata\n",
        "from random import randint\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import STOPWORDS, WordCloud\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, TimeDistributed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "handy-union",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:54:33.638375Z",
          "iopub.status.busy": "2021-04-05T01:54:33.624888Z",
          "iopub.status.idle": "2021-04-05T01:54:46.520965Z",
          "shell.execute_reply": "2021-04-05T01:54:46.520103Z"
        },
        "papermill": {
          "duration": 12.985933,
          "end_time": "2021-04-05T01:54:46.521107",
          "exception": false,
          "start_time": "2021-04-05T01:54:33.535174",
          "status": "completed"
        },
        "tags": [],
        "id": "handy-union"
      },
      "outputs": [],
      "source": [
        "!pip install -q contractions==0.0.48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scheduled-college",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:54:46.688231Z",
          "iopub.status.busy": "2021-04-05T01:54:46.687519Z",
          "iopub.status.idle": "2021-04-05T01:54:46.705103Z",
          "shell.execute_reply": "2021-04-05T01:54:46.704495Z"
        },
        "papermill": {
          "duration": 0.103127,
          "end_time": "2021-04-05T01:54:46.705252",
          "exception": false,
          "start_time": "2021-04-05T01:54:46.602125",
          "status": "completed"
        },
        "tags": [],
        "id": "scheduled-college"
      },
      "outputs": [],
      "source": [
        "from contractions import contractions_dict\n",
        "\n",
        "for key, value in list(contractions_dict.items())[:10]:\n",
        "    print(f'{key} == {value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mounted-imperial",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:54:46.879191Z",
          "iopub.status.busy": "2021-04-05T01:54:46.878491Z",
          "iopub.status.idle": "2021-04-05T01:54:52.227840Z",
          "shell.execute_reply": "2021-04-05T01:54:52.228393Z"
        },
        "papermill": {
          "duration": 5.44195,
          "end_time": "2021-04-05T01:54:52.228566",
          "exception": false,
          "start_time": "2021-04-05T01:54:46.786616",
          "status": "completed"
        },
        "tags": [],
        "id": "mounted-imperial"
      },
      "outputs": [],
      "source": [
        "# Using TPU\n",
        "\n",
        "# detect and init the TPU\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "# instantiate a distribution strategy\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "purple-cooling",
      "metadata": {
        "papermill": {
          "duration": 0.079663,
          "end_time": "2021-04-05T01:54:52.388651",
          "exception": false,
          "start_time": "2021-04-05T01:54:52.308988",
          "status": "completed"
        },
        "tags": [],
        "id": "purple-cooling"
      },
      "source": [
        "##  Getting the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(\"\", index=np.arange(31), columns=['text','headlines'])"
      ],
      "metadata": {
        "id": "oWzspkxirsto"
      },
      "execution_count": null,
      "outputs": [],
      "id": "oWzspkxirsto"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(path):\n",
        "  csv_file = pd.read_csv(path)\n",
        "  csv_file.dropna(axis = 0,inplace =True)\n",
        "  txt_op = \" \".join(csv_file['text'])\n",
        "  return txt_op"
      ],
      "metadata": {
        "id": "xcgBmGOiqPD5"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xcgBmGOiqPD5"
    },
    {
      "cell_type": "code",
      "source": [
        "path_text = \"/content/drive/MyDrive/raw_data\"\n",
        "path_summary = \"/content/drive/MyDrive/reference_summaries\""
      ],
      "metadata": {
        "id": "IKscM1ZmoZUT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IKscM1ZmoZUT"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,32):\n",
        "  df['text'].iloc[i-1] = get_text(path_text+\"/match_\"+str(i)+\"_comm.csv\")\n",
        "  if(i == 2): continue\n",
        "  summary_file = open(path_summary+\"/match_\"+str(i)+\"_report.txt\",\"r\")\n",
        "  lines = summary_file.readlines()\n",
        "  summary_file.close\n",
        "  df['headlines'].iloc[i-1] = str(lines[1])[1:-1]"
      ],
      "metadata": {
        "id": "4_8HOEcPoZWp"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4_8HOEcPoZWp"
    },
    {
      "cell_type": "markdown",
      "id": "freelance-program",
      "metadata": {
        "papermill": {
          "duration": 0.082642,
          "end_time": "2021-04-05T01:54:54.923015",
          "exception": false,
          "start_time": "2021-04-05T01:54:54.840373",
          "status": "completed"
        },
        "tags": [],
        "id": "freelance-program"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "binary-flower",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:54:55.112059Z",
          "iopub.status.busy": "2021-04-05T01:54:55.111081Z",
          "iopub.status.idle": "2021-04-05T01:54:55.115367Z",
          "shell.execute_reply": "2021-04-05T01:54:55.114886Z"
        },
        "papermill": {
          "duration": 0.106043,
          "end_time": "2021-04-05T01:54:55.115502",
          "exception": false,
          "start_time": "2021-04-05T01:54:55.009459",
          "status": "completed"
        },
        "tags": [],
        "id": "binary-flower"
      },
      "outputs": [],
      "source": [
        "def expand_contractions(text, contraction_map=contractions_dict):\n",
        "    # Using regex for getting all contracted words\n",
        "    contractions_keys = '|'.join(contraction_map.keys())\n",
        "    contractions_pattern = re.compile(f'({contractions_keys})', flags=re.DOTALL)\n",
        "\n",
        "    def expand_match(contraction):\n",
        "        # Getting entire matched sub-string\n",
        "        match = contraction.group(0)\n",
        "        expanded_contraction = contraction_map.get(match)\n",
        "        if not expand_contractions:\n",
        "            print(match)\n",
        "            return match\n",
        "        return expanded_contraction\n",
        "\n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "\n",
        "\n",
        "expand_contractions(\"y'all can't expand contractions i'd think\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coordinate-frequency",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:54:55.401964Z",
          "iopub.status.busy": "2021-04-05T01:54:55.288407Z",
          "iopub.status.idle": "2021-04-05T01:54:55.481579Z",
          "shell.execute_reply": "2021-04-05T01:54:55.480928Z"
        },
        "papermill": {
          "duration": 0.28355,
          "end_time": "2021-04-05T01:54:55.481732",
          "exception": false,
          "start_time": "2021-04-05T01:54:55.198182",
          "status": "completed"
        },
        "tags": [],
        "id": "coordinate-frequency"
      },
      "outputs": [],
      "source": [
        "# Converting to lowercase\n",
        "df.text = df.text.apply(str.lower)\n",
        "df.headlines = df.headlines.apply(str.lower)\n",
        "\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aerial-induction",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:54:55.663240Z",
          "iopub.status.busy": "2021-04-05T01:54:55.658093Z",
          "iopub.status.idle": "2021-04-05T01:55:40.814375Z",
          "shell.execute_reply": "2021-04-05T01:55:40.814913Z"
        },
        "papermill": {
          "duration": 45.247009,
          "end_time": "2021-04-05T01:55:40.815121",
          "exception": false,
          "start_time": "2021-04-05T01:54:55.568112",
          "status": "completed"
        },
        "tags": [],
        "id": "aerial-induction"
      },
      "outputs": [],
      "source": [
        "df.headlines = df.headlines.apply(expand_contractions)\n",
        "df.text = df.text.apply(expand_contractions)\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "protecting-cooperation",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:55:40.990109Z",
          "iopub.status.busy": "2021-04-05T01:55:40.989437Z",
          "iopub.status.idle": "2021-04-05T01:55:40.992862Z",
          "shell.execute_reply": "2021-04-05T01:55:40.993318Z"
        },
        "papermill": {
          "duration": 0.094385,
          "end_time": "2021-04-05T01:55:40.993488",
          "exception": false,
          "start_time": "2021-04-05T01:55:40.899103",
          "status": "completed"
        },
        "tags": [],
        "id": "protecting-cooperation"
      },
      "outputs": [],
      "source": [
        "# Remove puncuation from word\n",
        "def rm_punc_from_word(word):\n",
        "    clean_alphabet_list = [\n",
        "        alphabet for alphabet in word if alphabet not in string.punctuation\n",
        "    ]\n",
        "    return ''.join(clean_alphabet_list)\n",
        "\n",
        "print(rm_punc_from_word('#cool!'))\n",
        "\n",
        "\n",
        "# Remove puncuation from text\n",
        "def rm_punc_from_text(text):\n",
        "    clean_word_list = [rm_punc_from_word(word) for word in text]\n",
        "    return ''.join(clean_word_list)\n",
        "\n",
        "print(rm_punc_from_text(\"Frankly, my dear, I don't give a damn\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "broken-horror",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:55:41.171164Z",
          "iopub.status.busy": "2021-04-05T01:55:41.170505Z",
          "iopub.status.idle": "2021-04-05T01:55:41.174230Z",
          "shell.execute_reply": "2021-04-05T01:55:41.173593Z"
        },
        "papermill": {
          "duration": 0.09508,
          "end_time": "2021-04-05T01:55:41.174364",
          "exception": false,
          "start_time": "2021-04-05T01:55:41.079284",
          "status": "completed"
        },
        "tags": [],
        "id": "broken-horror"
      },
      "outputs": [],
      "source": [
        "# Remove numbers from text\n",
        "def rm_number_from_text(text):\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "    return ' '.join(text.split())  # to rm `extra` white space\n",
        "\n",
        "print(rm_number_from_text('You are 100times more sexier than me'))\n",
        "print(rm_number_from_text('If you taught yes then you are 10 times more delusional than me'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "catholic-nothing",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:55:41.352542Z",
          "iopub.status.busy": "2021-04-05T01:55:41.351901Z",
          "iopub.status.idle": "2021-04-05T01:55:41.363721Z",
          "shell.execute_reply": "2021-04-05T01:55:41.363016Z"
        },
        "papermill": {
          "duration": 0.103226,
          "end_time": "2021-04-05T01:55:41.363878",
          "exception": false,
          "start_time": "2021-04-05T01:55:41.260652",
          "status": "completed"
        },
        "tags": [],
        "id": "catholic-nothing"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords from text\\\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "def rm_stopwords_from_text(text):\n",
        "    _stopwords = stopwords.words('english')\n",
        "    text = text.split()\n",
        "    word_list = [word for word in text if word not in _stopwords]\n",
        "    return ' '.join(word_list)\n",
        "\n",
        "rm_stopwords_from_text(\"Love means never having to say you're sorry\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stylish-system",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:55:41.551188Z",
          "iopub.status.busy": "2021-04-05T01:55:41.549399Z",
          "iopub.status.idle": "2021-04-05T01:55:41.565224Z",
          "shell.execute_reply": "2021-04-05T01:55:41.564114Z"
        },
        "papermill": {
          "duration": 0.115638,
          "end_time": "2021-04-05T01:55:41.565387",
          "exception": false,
          "start_time": "2021-04-05T01:55:41.449749",
          "status": "completed"
        },
        "tags": [],
        "id": "stylish-system"
      },
      "outputs": [],
      "source": [
        "# Cleaning text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = rm_punc_from_text(text)\n",
        "    text = rm_number_from_text(text)\n",
        "    text = rm_stopwords_from_text(text)\n",
        "\n",
        "    # there are hyphen(–) in many titles, so replacing it with empty str\n",
        "    # this hyphen(–) is different from normal hyphen(-)\n",
        "    text = re.sub('–', '', text)\n",
        "    text = ' '.join(text.split())  # removing `extra` white spaces\n",
        "\n",
        "    # Removing unnecessary characters from text\n",
        "    text = re.sub(\"(\\\\t)\", ' ', str(text)).lower()\n",
        "    text = re.sub(\"(\\\\r)\", ' ', str(text)).lower()\n",
        "    text = re.sub(\"(\\\\n)\", ' ', str(text)).lower()\n",
        "\n",
        "    # remove accented chars ('Sómě Áccěntěd těxt' => 'Some Accented text')\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode(\n",
        "        'utf-8', 'ignore'\n",
        "    )\n",
        "\n",
        "    text = re.sub(\"(__+)\", ' ', str(text)).lower()\n",
        "    text = re.sub(\"(--+)\", ' ', str(text)).lower()\n",
        "    text = re.sub(\"(~~+)\", ' ', str(text)).lower()\n",
        "    text = re.sub(\"(\\+\\++)\", ' ', str(text)).lower()\n",
        "    text = re.sub(\"(\\.\\.+)\", ' ', str(text)).lower()\n",
        "\n",
        "    text = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(text)).lower()\n",
        "\n",
        "    text = re.sub(\"(mailto:)\", ' ', str(text)).lower()\n",
        "    text = re.sub(r\"(\\\\x9\\d)\", ' ', str(text)).lower()\n",
        "    text = re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(text)).lower()\n",
        "    text = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM',\n",
        "                  str(text)).lower()\n",
        "\n",
        "    text = re.sub(\"(\\.\\s+)\", ' ', str(text)).lower()\n",
        "    text = re.sub(\"(\\-\\s+)\", ' ', str(text)).lower()\n",
        "    text = re.sub(\"(\\:\\s+)\", ' ', str(text)).lower()\n",
        "    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n",
        "\n",
        "    try:\n",
        "        url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(text))\n",
        "        repl_url = url.group(3)\n",
        "        text = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', repl_url, str(text))\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    text = re.sub(\"(\\s+)\", ' ', str(text)).lower()\n",
        "    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "clean_text(\"Mrs. Robinson, you're trying to seduce me, aren't you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exclusive-heading",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:55:41.744938Z",
          "iopub.status.busy": "2021-04-05T01:55:41.744302Z",
          "iopub.status.idle": "2021-04-05T01:57:18.614930Z",
          "shell.execute_reply": "2021-04-05T01:57:18.615462Z"
        },
        "papermill": {
          "duration": 96.964518,
          "end_time": "2021-04-05T01:57:18.615675",
          "exception": false,
          "start_time": "2021-04-05T01:55:41.651157",
          "status": "completed"
        },
        "tags": [],
        "id": "exclusive-heading"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.apply(clean_text)\n",
        "df.headlines = df.headlines.apply(clean_text)\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "australian-making",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:57:18.793855Z",
          "iopub.status.busy": "2021-04-05T01:57:18.792953Z",
          "iopub.status.idle": "2021-04-05T01:57:20.333522Z",
          "shell.execute_reply": "2021-04-05T01:57:20.334165Z"
        },
        "papermill": {
          "duration": 1.632661,
          "end_time": "2021-04-05T01:57:20.334350",
          "exception": false,
          "start_time": "2021-04-05T01:57:18.701689",
          "status": "completed"
        },
        "tags": [],
        "id": "australian-making"
      },
      "outputs": [],
      "source": [
        "# saving the cleaned data\n",
        "df.to_csv('cleaned_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mighty-segment",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:57:20.512235Z",
          "iopub.status.busy": "2021-04-05T01:57:20.511282Z",
          "iopub.status.idle": "2021-04-05T01:57:20.520171Z",
          "shell.execute_reply": "2021-04-05T01:57:20.520699Z"
        },
        "papermill": {
          "duration": 0.099021,
          "end_time": "2021-04-05T01:57:20.520880",
          "exception": false,
          "start_time": "2021-04-05T01:57:20.421859",
          "status": "completed"
        },
        "tags": [],
        "id": "mighty-segment"
      },
      "outputs": [],
      "source": [
        "# To customize colours of wordcloud texts\n",
        "def wc_blue_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "    return \"hsl(214, 67%%, %d%%)\" % randint(60, 100)\n",
        "\n",
        "\n",
        "# stopwords for wordcloud\n",
        "def get_wc_stopwords():\n",
        "    wc_stopwords = set(STOPWORDS)\n",
        "\n",
        "    # Adding words to stopwords\n",
        "    # these words showed up while plotting wordcloud for text\n",
        "    wc_stopwords.add('s')\n",
        "    wc_stopwords.add('one')\n",
        "    wc_stopwords.add('using')\n",
        "    wc_stopwords.add('example')\n",
        "    wc_stopwords.add('work')\n",
        "    wc_stopwords.add('use')\n",
        "    wc_stopwords.add('make')\n",
        "\n",
        "    return wc_stopwords\n",
        "\n",
        "\n",
        "# plot wordcloud\n",
        "def plot_wordcloud(text, color_func):\n",
        "    wc_stopwords = get_wc_stopwords()\n",
        "    wc = WordCloud(stopwords=wc_stopwords, width=1200, height=600, random_state=0).generate(text)\n",
        "\n",
        "    f, axs = plt.subplots(figsize=(20, 10))\n",
        "    with sns.axes_style(\"ticks\"):\n",
        "        sns.despine(offset=10, trim=True)\n",
        "        plt.imshow(wc.recolor(color_func=color_func, random_state=0), interpolation=\"bilinear\")\n",
        "        plt.xlabel('WordCloud')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "distributed-matthew",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:57:20.704178Z",
          "iopub.status.busy": "2021-04-05T01:57:20.703214Z",
          "iopub.status.idle": "2021-04-05T01:57:30.843244Z",
          "shell.execute_reply": "2021-04-05T01:57:30.843752Z"
        },
        "papermill": {
          "duration": 10.231889,
          "end_time": "2021-04-05T01:57:30.843947",
          "exception": false,
          "start_time": "2021-04-05T01:57:20.612058",
          "status": "completed"
        },
        "tags": [],
        "id": "distributed-matthew"
      },
      "outputs": [],
      "source": [
        "plot_wordcloud(' '.join(df.headlines.values.tolist()), wc_blue_color_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "municipal-consultancy",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:57:31.060465Z",
          "iopub.status.busy": "2021-04-05T01:57:31.059768Z",
          "iopub.status.idle": "2021-04-05T01:58:06.441946Z",
          "shell.execute_reply": "2021-04-05T01:58:06.442468Z"
        },
        "papermill": {
          "duration": 35.492196,
          "end_time": "2021-04-05T01:58:06.442638",
          "exception": false,
          "start_time": "2021-04-05T01:57:30.950442",
          "status": "completed"
        },
        "tags": [],
        "id": "municipal-consultancy"
      },
      "outputs": [],
      "source": [
        "plot_wordcloud(' '.join(df.text.values.tolist()), wc_blue_color_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "three-element",
      "metadata": {
        "papermill": {
          "duration": 0.122164,
          "end_time": "2021-04-05T01:58:06.687389",
          "exception": false,
          "start_time": "2021-04-05T01:58:06.565225",
          "status": "completed"
        },
        "tags": [],
        "id": "three-element"
      },
      "source": [
        "Using a `start` and `end` tokens in `headlines(summary)` to let the learning algorithm know from where the headlines start's and end's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "amateur-writer",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:06.965547Z",
          "iopub.status.busy": "2021-04-05T01:58:06.939721Z",
          "iopub.status.idle": "2021-04-05T01:58:06.991703Z",
          "shell.execute_reply": "2021-04-05T01:58:06.991092Z"
        },
        "papermill": {
          "duration": 0.182131,
          "end_time": "2021-04-05T01:58:06.991867",
          "exception": false,
          "start_time": "2021-04-05T01:58:06.809736",
          "status": "completed"
        },
        "tags": [],
        "id": "amateur-writer"
      },
      "outputs": [],
      "source": [
        "df.headlines = df.headlines.apply(lambda x: f'_START_ {x} _END_')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pursuant-variety",
      "metadata": {
        "papermill": {
          "duration": 0.123424,
          "end_time": "2021-04-05T01:58:07.241075",
          "exception": false,
          "start_time": "2021-04-05T01:58:07.117651",
          "status": "completed"
        },
        "tags": [],
        "id": "pursuant-variety"
      },
      "source": [
        "Again adding `tokens` ... but different ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "constant-simpson",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:07.543473Z",
          "iopub.status.busy": "2021-04-05T01:58:07.517174Z",
          "iopub.status.idle": "2021-04-05T01:58:07.580146Z",
          "shell.execute_reply": "2021-04-05T01:58:07.579392Z"
        },
        "papermill": {
          "duration": 0.215826,
          "end_time": "2021-04-05T01:58:07.580310",
          "exception": false,
          "start_time": "2021-04-05T01:58:07.364484",
          "status": "completed"
        },
        "tags": [],
        "id": "constant-simpson"
      },
      "outputs": [],
      "source": [
        "start_token = 'sostok'\n",
        "end_token = 'eostok'\n",
        "df.headlines = df.headlines.apply(lambda x: f'{start_token} {x} {end_token}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "resident-greensboro",
      "metadata": {
        "papermill": {
          "duration": 0.122488,
          "end_time": "2021-04-05T01:58:07.826393",
          "exception": false,
          "start_time": "2021-04-05T01:58:07.703905",
          "status": "completed"
        },
        "tags": [],
        "id": "resident-greensboro"
      },
      "source": [
        "It's important to use `sostok` and `eostok` as start and end tokens respectively as later while using `tensorflow's Tokenizer` will filter the tokens and covert them to lowercase.\n",
        "\n",
        "**sostok** & **eostok** tokens are for us to know where to start & stop the summary because using `_START_` & `_END_`, tf's tokenizer with convert them to **start** & **end** respectively.\n",
        "\n",
        "So while decoding the summary sequences of sentences like **'everything is going to end in 2012'** if use `_START_` & `_END_` tokens (which will make the sentence like **'start everything is going to end in 2012 end'** this) whome tf's tokenizer will convert to start and end then we will stop decoding as we hit first **end**, so this is bad and therefore **sostok** & **eostok** these tokens are used.\n",
        "\n",
        "So we can just use these **sostok** & **eostok** instead of `_START_` & `_END_`, well you can but I tried both ways and while not using these `_START_` & `_END_` I was getting `undesired results` 🤯 😅 i.e. model's `results weren't good`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cultural-capital",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:08.087849Z",
          "iopub.status.busy": "2021-04-05T01:58:08.087193Z",
          "iopub.status.idle": "2021-04-05T01:58:08.091014Z",
          "shell.execute_reply": "2021-04-05T01:58:08.090449Z"
        },
        "papermill": {
          "duration": 0.141392,
          "end_time": "2021-04-05T01:58:08.091158",
          "exception": false,
          "start_time": "2021-04-05T01:58:07.949766",
          "status": "completed"
        },
        "tags": [],
        "id": "cultural-capital"
      },
      "outputs": [],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "royal-highway",
      "metadata": {
        "papermill": {
          "duration": 0.123257,
          "end_time": "2021-04-05T01:58:08.338481",
          "exception": false,
          "start_time": "2021-04-05T01:58:08.215224",
          "status": "completed"
        },
        "tags": [],
        "id": "royal-highway"
      },
      "source": [
        "Finding what should be the `maximum length` of text and headlines that will be feed or accepted by the learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "organizational-lighter",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:08.600456Z",
          "iopub.status.busy": "2021-04-05T01:58:08.595212Z",
          "iopub.status.idle": "2021-04-05T01:58:09.680367Z",
          "shell.execute_reply": "2021-04-05T01:58:09.679845Z"
        },
        "papermill": {
          "duration": 1.218528,
          "end_time": "2021-04-05T01:58:09.680507",
          "exception": false,
          "start_time": "2021-04-05T01:58:08.461979",
          "status": "completed"
        },
        "tags": [],
        "id": "organizational-lighter"
      },
      "outputs": [],
      "source": [
        "text_count = [len(sentence.split()) for sentence in df.text]\n",
        "headlines_count = [len(sentence.split()) for sentence in df.headlines]\n",
        "\n",
        "pd.DataFrame({'text': text_count, 'headlines': headlines_count}).hist(bins=100, figsize=(16, 4), range=[0, 5000])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comprehensive-insight",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:09.955522Z",
          "iopub.status.busy": "2021-04-05T01:58:09.950466Z",
          "iopub.status.idle": "2021-04-05T01:58:10.324956Z",
          "shell.execute_reply": "2021-04-05T01:58:10.325703Z"
        },
        "papermill": {
          "duration": 0.52064,
          "end_time": "2021-04-05T01:58:10.325949",
          "exception": false,
          "start_time": "2021-04-05T01:58:09.805309",
          "status": "completed"
        },
        "tags": [],
        "id": "comprehensive-insight"
      },
      "outputs": [],
      "source": [
        "# To check how many rows in a column has length (of the text) <= limit\n",
        "def get_word_percent(column, limit):\n",
        "    count = 0\n",
        "    for sentence in column:\n",
        "        if len(sentence.split()) <= limit:\n",
        "            count += 1\n",
        "\n",
        "    return round(count / len(column), 2)\n",
        "\n",
        "\n",
        "# Check how many % of headlines have 0-13 words\n",
        "print(get_word_percent(df.headlines, 500))\n",
        "\n",
        "# Check how many % of summary have 0-42 words\n",
        "print(get_word_percent(df.text, 2800))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stopped-tattoo",
      "metadata": {
        "papermill": {
          "duration": 0.126709,
          "end_time": "2021-04-05T01:58:10.578329",
          "exception": false,
          "start_time": "2021-04-05T01:58:10.451620",
          "status": "completed"
        },
        "tags": [],
        "id": "stopped-tattoo"
      },
      "source": [
        "If the length of headlines or the text is kept large the deep learning model will face issues with performance and also training will slower.\n",
        "\n",
        "One solution for creating summary for long sentences can be break a paragraph into sentences and then create a summary for them, this way the summary will make sence instead of giving random piece of text and creating summary for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hollywood-coffee",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:10.834259Z",
          "iopub.status.busy": "2021-04-05T01:58:10.833418Z",
          "iopub.status.idle": "2021-04-05T01:58:10.837275Z",
          "shell.execute_reply": "2021-04-05T01:58:10.836652Z"
        },
        "papermill": {
          "duration": 0.134234,
          "end_time": "2021-04-05T01:58:10.837406",
          "exception": false,
          "start_time": "2021-04-05T01:58:10.703172",
          "status": "completed"
        },
        "tags": [],
        "id": "hollywood-coffee"
      },
      "outputs": [],
      "source": [
        "max_text_len = 2800\n",
        "max_summary_len = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "boxed-spare",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:11.100843Z",
          "iopub.status.busy": "2021-04-05T01:58:11.098651Z",
          "iopub.status.idle": "2021-04-05T01:58:11.565206Z",
          "shell.execute_reply": "2021-04-05T01:58:11.564476Z"
        },
        "papermill": {
          "duration": 0.602118,
          "end_time": "2021-04-05T01:58:11.565364",
          "exception": false,
          "start_time": "2021-04-05T01:58:10.963246",
          "status": "completed"
        },
        "tags": [],
        "id": "boxed-spare"
      },
      "outputs": [],
      "source": [
        "# select the summary and text between their defined max lens respectively\n",
        "def trim_text_and_summary(df, max_text_len, max_summary_len):\n",
        "    cleaned_text = np.array(df['text'])\n",
        "    cleaned_summary = np.array(df['headlines'])\n",
        "\n",
        "    short_text = []\n",
        "    short_summary = []\n",
        "\n",
        "    for i in range(len(cleaned_text)):\n",
        "        if len(cleaned_text[i].split()) <= max_text_len and len(\n",
        "            cleaned_summary[i].split()\n",
        "        ) <= max_summary_len:\n",
        "            short_text.append(cleaned_text[i])\n",
        "            short_summary.append(cleaned_summary[i])\n",
        "\n",
        "    df = pd.DataFrame({'text': short_text, 'summary': short_summary})\n",
        "    return df\n",
        "\n",
        "\n",
        "df = trim_text_and_summary(df, max_text_len, max_summary_len)\n",
        "print(f'Dataset size: {len(df)}')\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aboriginal-trace",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:11.832933Z",
          "iopub.status.busy": "2021-04-05T01:58:11.831988Z",
          "iopub.status.idle": "2021-04-05T01:58:11.834810Z",
          "shell.execute_reply": "2021-04-05T01:58:11.834163Z"
        },
        "papermill": {
          "duration": 0.142062,
          "end_time": "2021-04-05T01:58:11.834948",
          "exception": false,
          "start_time": "2021-04-05T01:58:11.692886",
          "status": "completed"
        },
        "tags": [],
        "id": "aboriginal-trace"
      },
      "outputs": [],
      "source": [
        "# rare word analysis\n",
        "def get_rare_word_percent(tokenizer, threshold):\n",
        "    # threshold: if the word's occurrence is less than this then it's rare word\n",
        "\n",
        "    count = 0\n",
        "    total_count = 0\n",
        "    frequency = 0\n",
        "    total_frequency = 0\n",
        "\n",
        "    for key, value in tokenizer.word_counts.items():\n",
        "        total_count += 1\n",
        "        total_frequency += value\n",
        "        if value < threshold:\n",
        "            count += 1\n",
        "            frequency += value\n",
        "\n",
        "    return {\n",
        "        'percent': round((count / total_count) * 100, 2),\n",
        "        'total_coverage': round(frequency / total_frequency * 100, 2),\n",
        "        'count': count,\n",
        "        'total_count': total_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "following-tackle",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:12.354446Z",
          "iopub.status.busy": "2021-04-05T01:58:12.352238Z",
          "iopub.status.idle": "2021-04-05T01:58:12.375114Z",
          "shell.execute_reply": "2021-04-05T01:58:12.374519Z"
        },
        "papermill": {
          "duration": 0.154878,
          "end_time": "2021-04-05T01:58:12.375255",
          "exception": false,
          "start_time": "2021-04-05T01:58:12.220377",
          "status": "completed"
        },
        "tags": [],
        "id": "following-tackle"
      },
      "outputs": [],
      "source": [
        "# Splitting the training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    np.array(df['text']),\n",
        "    np.array(df['summary']),\n",
        "    test_size=0.1,\n",
        "    random_state=1,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "african-upset",
      "metadata": {
        "papermill": {
          "duration": 0.125868,
          "end_time": "2021-04-05T01:58:12.627774",
          "exception": false,
          "start_time": "2021-04-05T01:58:12.501906",
          "status": "completed"
        },
        "tags": [],
        "id": "african-upset"
      },
      "source": [
        "**Tokenizing text -> x**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "capital-setting",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:12.884469Z",
          "iopub.status.busy": "2021-04-05T01:58:12.883609Z",
          "iopub.status.idle": "2021-04-05T01:58:16.924133Z",
          "shell.execute_reply": "2021-04-05T01:58:16.923308Z"
        },
        "papermill": {
          "duration": 4.170136,
          "end_time": "2021-04-05T01:58:16.924357",
          "exception": false,
          "start_time": "2021-04-05T01:58:12.754221",
          "status": "completed"
        },
        "tags": [],
        "id": "capital-setting"
      },
      "outputs": [],
      "source": [
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "\n",
        "x_tokens_data = get_rare_word_percent(x_tokenizer, 4)\n",
        "print(x_tokens_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acceptable-comment",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:17.486484Z",
          "iopub.status.busy": "2021-04-05T01:58:17.475973Z",
          "iopub.status.idle": "2021-04-05T01:58:21.369213Z",
          "shell.execute_reply": "2021-04-05T01:58:21.368493Z"
        },
        "papermill": {
          "duration": 4.058228,
          "end_time": "2021-04-05T01:58:21.369359",
          "exception": false,
          "start_time": "2021-04-05T01:58:17.311131",
          "status": "completed"
        },
        "tags": [],
        "id": "acceptable-comment"
      },
      "outputs": [],
      "source": [
        "# else use this\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collaborative-composition",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:21.634143Z",
          "iopub.status.busy": "2021-04-05T01:58:21.631245Z",
          "iopub.status.idle": "2021-04-05T01:58:21.779229Z",
          "shell.execute_reply": "2021-04-05T01:58:21.778487Z"
        },
        "papermill": {
          "duration": 0.281341,
          "end_time": "2021-04-05T01:58:21.779395",
          "exception": false,
          "start_time": "2021-04-05T01:58:21.498054",
          "status": "completed"
        },
        "tags": [],
        "id": "collaborative-composition"
      },
      "outputs": [],
      "source": [
        "# save tokenizer\n",
        "with open('x_tokenizer', 'wb') as f:\n",
        "    pickle.dump(x_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ignored-scheme",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:22.054549Z",
          "iopub.status.busy": "2021-04-05T01:58:22.049208Z",
          "iopub.status.idle": "2021-04-05T01:58:26.580186Z",
          "shell.execute_reply": "2021-04-05T01:58:26.580654Z"
        },
        "papermill": {
          "duration": 4.673769,
          "end_time": "2021-04-05T01:58:26.580843",
          "exception": false,
          "start_time": "2021-04-05T01:58:21.907074",
          "status": "completed"
        },
        "tags": [],
        "id": "ignored-scheme"
      },
      "outputs": [],
      "source": [
        "# one-hot-encoding\n",
        "x_train_sequence = x_tokenizer.texts_to_sequences(x_train)\n",
        "x_val_sequence = x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "# padding upto max_text_len\n",
        "x_train_padded = pad_sequences(x_train_sequence, maxlen=max_text_len, padding='post')\n",
        "x_val_padded = pad_sequences(x_val_sequence, maxlen=max_text_len, padding='post')\n",
        "\n",
        "# if you're not using num_words parameter in Tokenizer then use this\n",
        "x_vocab_size = len(x_tokenizer.word_index) + 1\n",
        "\n",
        "# else use this\n",
        "# x_vocab_size = x_tokenizer.num_words + 1\n",
        "\n",
        "print(x_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sapphire-checklist",
      "metadata": {
        "papermill": {
          "duration": 0.127763,
          "end_time": "2021-04-05T01:58:26.835928",
          "exception": false,
          "start_time": "2021-04-05T01:58:26.708165",
          "status": "completed"
        },
        "tags": [],
        "id": "sapphire-checklist"
      },
      "source": [
        "**Tokenizing headlines(summary) 👉 y**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "moving-beaver",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:27.130250Z",
          "iopub.status.busy": "2021-04-05T01:58:27.129252Z",
          "iopub.status.idle": "2021-04-05T01:58:28.873565Z",
          "shell.execute_reply": "2021-04-05T01:58:28.874084Z"
        },
        "papermill": {
          "duration": 1.908165,
          "end_time": "2021-04-05T01:58:28.874275",
          "exception": false,
          "start_time": "2021-04-05T01:58:26.966110",
          "status": "completed"
        },
        "tags": [],
        "id": "moving-beaver"
      },
      "outputs": [],
      "source": [
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "\n",
        "y_tokens_data = get_rare_word_percent(y_tokenizer, 6)\n",
        "print(y_tokens_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "periodic-accreditation",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:29.465757Z",
          "iopub.status.busy": "2021-04-05T01:58:29.449912Z",
          "iopub.status.idle": "2021-04-05T01:58:31.176489Z",
          "shell.execute_reply": "2021-04-05T01:58:31.175902Z"
        },
        "papermill": {
          "duration": 1.912597,
          "end_time": "2021-04-05T01:58:31.176633",
          "exception": false,
          "start_time": "2021-04-05T01:58:29.264036",
          "status": "completed"
        },
        "tags": [],
        "id": "periodic-accreditation"
      },
      "outputs": [],
      "source": [
        "# else use this\n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "honey-establishment",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:31.440281Z",
          "iopub.status.busy": "2021-04-05T01:58:31.439578Z",
          "iopub.status.idle": "2021-04-05T01:58:31.486965Z",
          "shell.execute_reply": "2021-04-05T01:58:31.487463Z"
        },
        "papermill": {
          "duration": 0.181487,
          "end_time": "2021-04-05T01:58:31.487646",
          "exception": false,
          "start_time": "2021-04-05T01:58:31.306159",
          "status": "completed"
        },
        "tags": [],
        "id": "honey-establishment"
      },
      "outputs": [],
      "source": [
        "# save tokenizer\n",
        "with open('y_tokenizer', 'wb') as f:\n",
        "    pickle.dump(y_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "received-photography",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:31.751826Z",
          "iopub.status.busy": "2021-04-05T01:58:31.750732Z",
          "iopub.status.idle": "2021-04-05T01:58:34.345846Z",
          "shell.execute_reply": "2021-04-05T01:58:34.346767Z"
        },
        "papermill": {
          "duration": 2.730404,
          "end_time": "2021-04-05T01:58:34.347092",
          "exception": false,
          "start_time": "2021-04-05T01:58:31.616688",
          "status": "completed"
        },
        "tags": [],
        "id": "received-photography"
      },
      "outputs": [],
      "source": [
        "# one-hot-encoding\n",
        "y_train_sequence = y_tokenizer.texts_to_sequences(y_train)\n",
        "y_val_sequence = y_tokenizer.texts_to_sequences(y_val)\n",
        "\n",
        "# padding upto max_summary_len\n",
        "y_train_padded = pad_sequences(y_train_sequence, maxlen=max_summary_len, padding='post')\n",
        "y_val_padded = pad_sequences(y_val_sequence, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "# if you're not using num_words parameter in Tokenizer then use this\n",
        "y_vocab_size = len(y_tokenizer.word_index) + 1\n",
        "\n",
        "# else use this\n",
        "# y_vocab_size = y_tokenizer.num_words + 1\n",
        "\n",
        "print(y_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "historical-estate",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:34.612000Z",
          "iopub.status.busy": "2021-04-05T01:58:34.611349Z",
          "iopub.status.idle": "2021-04-05T01:58:38.756280Z",
          "shell.execute_reply": "2021-04-05T01:58:38.756772Z"
        },
        "papermill": {
          "duration": 4.278314,
          "end_time": "2021-04-05T01:58:38.756978",
          "exception": false,
          "start_time": "2021-04-05T01:58:34.478664",
          "status": "completed"
        },
        "tags": [],
        "id": "historical-estate"
      },
      "outputs": [],
      "source": [
        "# removing summary which only has sostok & eostok\n",
        "def remove_indexes(summary_array):\n",
        "    remove_indexes = []\n",
        "    for i in range(len(summary_array)):\n",
        "        count = 0\n",
        "        for j in summary_array[i]:\n",
        "            if j != 0:\n",
        "                count += 1\n",
        "        if count == 2:\n",
        "            remove_indexes.append(i)\n",
        "    return remove_indexes\n",
        "\n",
        "\n",
        "remove_train_indexes = remove_indexes(y_train_padded)\n",
        "remove_val_indexes = remove_indexes(y_val_padded)\n",
        "\n",
        "y_train_padded = np.delete(y_train_padded, remove_train_indexes, axis=0)\n",
        "x_train_padded = np.delete(x_train_padded, remove_train_indexes, axis=0)\n",
        "\n",
        "y_val_padded = np.delete(y_val_padded, remove_val_indexes, axis=0)\n",
        "x_val_padded = np.delete(x_val_padded, remove_val_indexes, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "disabled-vitamin",
      "metadata": {
        "papermill": {
          "duration": 0.129921,
          "end_time": "2021-04-05T01:58:39.017520",
          "exception": false,
          "start_time": "2021-04-05T01:58:38.887599",
          "status": "completed"
        },
        "tags": [],
        "id": "disabled-vitamin"
      },
      "source": [
        "##  Modelling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solved-niger",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:39.280422Z",
          "iopub.status.busy": "2021-04-05T01:58:39.279803Z",
          "iopub.status.idle": "2021-04-05T01:58:39.284058Z",
          "shell.execute_reply": "2021-04-05T01:58:39.284513Z"
        },
        "papermill": {
          "duration": 0.136696,
          "end_time": "2021-04-05T01:58:39.284676",
          "exception": false,
          "start_time": "2021-04-05T01:58:39.147980",
          "status": "completed"
        },
        "tags": [],
        "id": "solved-niger"
      },
      "outputs": [],
      "source": [
        "latent_dim = 240\n",
        "embedding_dim = 300\n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "id": "UuLfNSc300Z7"
      },
      "id": "UuLfNSc300Z7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "dzy06sFZ05Gh"
      },
      "id": "dzy06sFZ05Gh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!pwd"
      ],
      "metadata": {
        "id": "Lpf9zo801gfM"
      },
      "id": "Lpf9zo801gfM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "returning-lyric",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:58:39.557168Z",
          "iopub.status.busy": "2021-04-05T01:58:39.556446Z",
          "iopub.status.idle": "2021-04-05T01:59:56.669421Z",
          "shell.execute_reply": "2021-04-05T01:59:56.669954Z"
        },
        "papermill": {
          "duration": 77.255427,
          "end_time": "2021-04-05T01:59:56.670128",
          "exception": false,
          "start_time": "2021-04-05T01:58:39.414701",
          "status": "completed"
        },
        "tags": [],
        "id": "returning-lyric"
      },
      "outputs": [],
      "source": [
        "def get_embedding_matrix(tokenizer, embedding_dim, vocab_size=None):\n",
        "    word_index = tokenizer.word_index\n",
        "    voc = list(word_index.keys())\n",
        "\n",
        "    path_to_glove_file = '/content/glove.6B.300d.txt'\n",
        "\n",
        "    embeddings_index = {}\n",
        "    with open(path_to_glove_file) as f:\n",
        "        for line in f:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    print(\"Found %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "    num_tokens = len(voc) + 2 if not vocab_size else vocab_size\n",
        "    hits = 0\n",
        "    misses = 0\n",
        "\n",
        "    # Prepare embedding matrix\n",
        "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            # This includes the representation for \"padding\" and \"OOV\"\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            hits += 1\n",
        "        else:\n",
        "            misses += 1\n",
        "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "x_embedding_matrix = get_embedding_matrix(x_tokenizer, embedding_dim, x_vocab_size)\n",
        "y_embedding_matrix = get_embedding_matrix(y_tokenizer, embedding_dim, y_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "serial-lincoln",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:59:56.936405Z",
          "iopub.status.busy": "2021-04-05T01:59:56.935698Z",
          "iopub.status.idle": "2021-04-05T01:59:56.939255Z",
          "shell.execute_reply": "2021-04-05T01:59:56.939758Z"
        },
        "papermill": {
          "duration": 0.139914,
          "end_time": "2021-04-05T01:59:56.939952",
          "exception": false,
          "start_time": "2021-04-05T01:59:56.800038",
          "status": "completed"
        },
        "tags": [],
        "id": "serial-lincoln"
      },
      "outputs": [],
      "source": [
        "print(x_embedding_matrix.shape)\n",
        "print(y_embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sufficient-czech",
      "metadata": {
        "papermill": {
          "duration": 0.132759,
          "end_time": "2021-04-05T01:59:57.254094",
          "exception": false,
          "start_time": "2021-04-05T01:59:57.121335",
          "status": "completed"
        },
        "tags": [],
        "id": "sufficient-czech"
      },
      "source": [
        "Using `pre-trained` embeddings and keeping the `Embedding` layer `non-trainable` we get increase in computation speed as don't need to compute the embedding matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "attractive-impression",
      "metadata": {
        "papermill": {
          "duration": 0.17018,
          "end_time": "2021-04-05T01:59:57.556232",
          "exception": false,
          "start_time": "2021-04-05T01:59:57.386052",
          "status": "completed"
        },
        "tags": [],
        "id": "attractive-impression"
      },
      "source": [
        "**Here there 3 different training models**\n",
        "- `build_seq2seq_model_with_just_lstm` - **Seq2Seq model with just LSTMs**. Both `encoder` and `decoder` have just `LSTM`s.\n",
        "- `build_seq2seq_model_with_bidirectional_lstm` - **Seq2Seq model with Bidirectional LSTMs**. Both `encoder` and `decoder` have `Bidirectional LSTM`s.\n",
        "- `build_hybrid_seq2seq_model` - **Seq2Seq model with hybrid architecture**. Here `encoder` has `Bidirectional LSTM`s while `decoder` has just `LSTM`s.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vertical-membership",
      "metadata": {
        "papermill": {
          "duration": 0.140293,
          "end_time": "2021-04-05T01:59:57.833110",
          "exception": false,
          "start_time": "2021-04-05T01:59:57.692817",
          "status": "completed"
        },
        "tags": [],
        "id": "vertical-membership"
      },
      "source": [
        "**Seq2Seq model with just LSTMs**. Both `encoder` and `decoder` have just `LSTM`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "horizontal-sacramento",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:59:58.144898Z",
          "iopub.status.busy": "2021-04-05T01:59:58.144164Z",
          "iopub.status.idle": "2021-04-05T01:59:58.147946Z",
          "shell.execute_reply": "2021-04-05T01:59:58.147285Z"
        },
        "papermill": {
          "duration": 0.165554,
          "end_time": "2021-04-05T01:59:58.148099",
          "exception": false,
          "start_time": "2021-04-05T01:59:57.982545",
          "status": "completed"
        },
        "tags": [],
        "id": "horizontal-sacramento"
      },
      "outputs": [],
      "source": [
        "def build_seq2seq_model_with_just_lstm(\n",
        "    embedding_dim, latent_dim, max_text_len, \n",
        "    x_vocab_size, y_vocab_size,\n",
        "    x_embedding_matrix, y_embedding_matrix\n",
        "):\n",
        "    # instantiating the model in the strategy scope creates the model on the TPU\n",
        "    with tpu_strategy.scope():\n",
        "\n",
        "        # =====================\n",
        "        #  Encoder\n",
        "        # =====================\n",
        "        encoder_input = Input(shape=(max_text_len, ))\n",
        "\n",
        "        # encoder embedding layer\n",
        "        encoder_embedding = Embedding(\n",
        "            x_vocab_size,\n",
        "            embedding_dim,\n",
        "            embeddings_initializer=tf.keras.initializers.Constant(x_embedding_matrix),\n",
        "            trainable=False\n",
        "        )(encoder_input)\n",
        "\n",
        "        # encoder lstm 1\n",
        "        encoder_lstm1 = LSTM(\n",
        "            latent_dim,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            dropout=0.4,\n",
        "            recurrent_dropout=0.4\n",
        "        )\n",
        "        encoder_output1, state_h1, state_c1 = encoder_lstm1(encoder_embedding)\n",
        "\n",
        "        # encoder lstm 2\n",
        "        encoder_lstm2 = LSTM(\n",
        "            latent_dim,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            dropout=0.4,\n",
        "            recurrent_dropout=0.4\n",
        "        )\n",
        "        encoder_output, *encoder_final_states = encoder_lstm2(encoder_output1)\n",
        "\n",
        "        # =====================\n",
        "        #  Decoder\n",
        "        # =====================\n",
        "\n",
        "        # Set up the decoder, using `encoder_states` as initial state.\n",
        "\n",
        "        decoder_input = Input(shape=(None, ))\n",
        "\n",
        "        # decoder embedding layer\n",
        "        decoder_embedding_layer = Embedding(\n",
        "            y_vocab_size,\n",
        "            embedding_dim,\n",
        "            embeddings_initializer=tf.keras.initializers.Constant(y_embedding_matrix),\n",
        "            trainable=True\n",
        "        )\n",
        "        decoder_embedding = decoder_embedding_layer(decoder_input)\n",
        "\n",
        "        # decoder lstm 1\n",
        "        decoder_lstm = LSTM(\n",
        "            latent_dim,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            dropout=0.4,\n",
        "            recurrent_dropout=0.4\n",
        "        )\n",
        "        decoder_output, *decoder_final_states = decoder_lstm(\n",
        "            decoder_embedding, initial_state=encoder_final_states\n",
        "        )\n",
        "\n",
        "        # dense layer\n",
        "        decoder_dense = TimeDistributed(\n",
        "            Dense(y_vocab_size, activation='softmax')\n",
        "        )\n",
        "        decoder_output = decoder_dense(decoder_output)\n",
        "\n",
        "        # =====================\n",
        "        #  Model\n",
        "        # =====================\n",
        "        model = Model([encoder_input, decoder_input], decoder_output)\n",
        "        model.summary()\n",
        "\n",
        "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'model': model,\n",
        "            'inputs': {\n",
        "                'encoder': encoder_input,\n",
        "                'decoder': decoder_input\n",
        "            },\n",
        "            'outputs': {\n",
        "                'encoder': encoder_output,\n",
        "                'decoder': decoder_output\n",
        "            },\n",
        "            'states': {\n",
        "                'encoder': encoder_final_states,\n",
        "                'decoder': decoder_final_states\n",
        "            },\n",
        "            'layers': {\n",
        "                'decoder': {\n",
        "                    'embedding': decoder_embedding_layer,\n",
        "                    'last_decoder_lstm': decoder_lstm,\n",
        "                    'dense': decoder_dense\n",
        "                }\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wired-testing",
      "metadata": {
        "papermill": {
          "duration": 0.134909,
          "end_time": "2021-04-05T01:59:58.420854",
          "exception": false,
          "start_time": "2021-04-05T01:59:58.285945",
          "status": "completed"
        },
        "tags": [],
        "id": "wired-testing"
      },
      "source": [
        "**Seq2Seq model with Bidirectional LSTMs**. Both `encoder` and `decoder` have `Bidirectional LSTM`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mental-niger",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:59:58.693537Z",
          "iopub.status.busy": "2021-04-05T01:59:58.692435Z",
          "iopub.status.idle": "2021-04-05T01:59:58.709808Z",
          "shell.execute_reply": "2021-04-05T01:59:58.710336Z"
        },
        "papermill": {
          "duration": 0.153024,
          "end_time": "2021-04-05T01:59:58.710509",
          "exception": false,
          "start_time": "2021-04-05T01:59:58.557485",
          "status": "completed"
        },
        "tags": [],
        "id": "mental-niger"
      },
      "outputs": [],
      "source": [
        "def build_seq2seq_model_with_bidirectional_lstm(\n",
        "    embedding_dim, latent_dim, max_text_len, \n",
        "    x_vocab_size, y_vocab_size,\n",
        "    x_embedding_matrix, y_embedding_matrix\n",
        "):\n",
        "    # instantiating the model in the strategy scope creates the model on the TPU\n",
        "    with tpu_strategy.scope():\n",
        "\n",
        "        # =====================\n",
        "        #  Encoder\n",
        "        # =====================\n",
        "        encoder_input = Input(shape=(max_text_len, ))\n",
        "\n",
        "        # encoder embedding layer\n",
        "        encoder_embedding = Embedding(\n",
        "            x_vocab_size,\n",
        "            embedding_dim,\n",
        "            embeddings_initializer=tf.keras.initializers.Constant(x_embedding_matrix),\n",
        "            trainable=False,\n",
        "            name='encoder_embedding'\n",
        "        )(encoder_input)\n",
        "\n",
        "        # encoder lstm1\n",
        "        encoder_bi_lstm1 = Bidirectional(\n",
        "            LSTM(\n",
        "                latent_dim,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                dropout=0.4,\n",
        "                recurrent_dropout=0.4,\n",
        "                name='encoder_lstm_1'\n",
        "            ),\n",
        "            name='encoder_bidirectional_lstm_1'\n",
        "        )\n",
        "        encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_bi_lstm1(\n",
        "            encoder_embedding\n",
        "        )\n",
        "        encoder_bi_lstm1_output = [\n",
        "            encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1\n",
        "        ]\n",
        "\n",
        "        # encoder lstm 2\n",
        "        encoder_bi_lstm2 = Bidirectional(\n",
        "            LSTM(\n",
        "                latent_dim,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                dropout=0.4,\n",
        "                recurrent_dropout=0.4,\n",
        "                name='encoder_lstm_2'\n",
        "            ),\n",
        "            name='encoder_bidirectional_lstm_2'\n",
        "        )\n",
        "        encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_bi_lstm2(\n",
        "            encoder_output1\n",
        "        )\n",
        "        encoder_bi_lstm2_output = [\n",
        "            encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2\n",
        "        ]\n",
        "\n",
        "        # encoder lstm 3\n",
        "        encoder_bi_lstm = Bidirectional(\n",
        "            LSTM(\n",
        "                latent_dim,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                dropout=0.4,\n",
        "                recurrent_dropout=0.4,\n",
        "                name='encoder_lstm_3'\n",
        "            ),\n",
        "            name='encoder_bidirectional_lstm_3'\n",
        "        )\n",
        "        encoder_output, *encoder_final_states = encoder_bi_lstm(encoder_output2)\n",
        "\n",
        "        # =====================\n",
        "        #  Decoder\n",
        "        # =====================\n",
        "\n",
        "        # Set up the decoder, using `encoder_states` as initial state.\n",
        "\n",
        "        decoder_input = Input(shape=(None, ))\n",
        "\n",
        "        # decoder embedding layer\n",
        "        decoder_embedding_layer = Embedding(\n",
        "            y_vocab_size,\n",
        "            embedding_dim,\n",
        "            embeddings_initializer=tf.keras.initializers.Constant(y_embedding_matrix),\n",
        "            trainable=False,\n",
        "            name='decoder_embedding'\n",
        "        )\n",
        "        decoder_embedding = decoder_embedding_layer(decoder_input)\n",
        "        \n",
        "        decoder_bi_lstm = Bidirectional(\n",
        "            LSTM(\n",
        "                latent_dim,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                dropout=0.4,\n",
        "                recurrent_dropout=0.2,\n",
        "                name='decoder_lstm_1'\n",
        "            ),\n",
        "            name='decoder_bidirectional_lstm_1'\n",
        "        )\n",
        "        decoder_output, *decoder_final_states = decoder_bi_lstm(\n",
        "            decoder_embedding, initial_state=encoder_final_states\n",
        "            # decoder_embedding, initial_state=encoder_final_states[:2]\n",
        "        )  # taking only the forward states\n",
        "\n",
        "        # dense layer\n",
        "        decoder_dense = TimeDistributed(\n",
        "            Dense(y_vocab_size, activation='softmax')\n",
        "        )\n",
        "        decoder_output = decoder_dense(decoder_output)\n",
        "\n",
        "        # =====================\n",
        "        #  Model\n",
        "        # =====================\n",
        "        model = Model([encoder_input, decoder_input], decoder_output, name='seq2seq_model_with_bidirectional_lstm')\n",
        "        model.summary()\n",
        "\n",
        "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'model': model,\n",
        "            'inputs': {\n",
        "                'encoder': encoder_input,\n",
        "                'decoder': decoder_input\n",
        "            },\n",
        "            'outputs': {\n",
        "                'encoder': encoder_output,\n",
        "                'decoder': decoder_output\n",
        "            },\n",
        "            'states': {\n",
        "                'encoder': encoder_final_states,\n",
        "                'decoder': decoder_final_states\n",
        "            },\n",
        "            'layers': {\n",
        "                'decoder': {\n",
        "                    'embedding': decoder_embedding_layer,\n",
        "                    'last_decoder_lstm': decoder_bi_lstm,\n",
        "                    'dense': decoder_dense\n",
        "                }\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "about-prince",
      "metadata": {
        "papermill": {
          "duration": 0.13135,
          "end_time": "2021-04-05T01:59:58.973341",
          "exception": false,
          "start_time": "2021-04-05T01:59:58.841991",
          "status": "completed"
        },
        "tags": [],
        "id": "about-prince"
      },
      "source": [
        "**Seq2Seq model with hybrid architecture**. Here `encoder` has `Bidirectional LSTM`s while `decoder` has just `LSTM`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "working-louis",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:59:59.245126Z",
          "iopub.status.busy": "2021-04-05T01:59:59.244033Z",
          "iopub.status.idle": "2021-04-05T01:59:59.260849Z",
          "shell.execute_reply": "2021-04-05T01:59:59.261330Z"
        },
        "papermill": {
          "duration": 0.15469,
          "end_time": "2021-04-05T01:59:59.261509",
          "exception": false,
          "start_time": "2021-04-05T01:59:59.106819",
          "status": "completed"
        },
        "tags": [],
        "id": "working-louis"
      },
      "outputs": [],
      "source": [
        "def build_hybrid_seq2seq_model(\n",
        "    embedding_dim, latent_dim, max_text_len, \n",
        "    x_vocab_size, y_vocab_size,\n",
        "    x_embedding_matrix, y_embedding_matrix\n",
        "):\n",
        "    # instantiating the model in the strategy scope creates the model on the TPU\n",
        "    with tpu_strategy.scope():\n",
        "\n",
        "        # =====================\n",
        "        #  Encoder\n",
        "        # =====================\n",
        "        encoder_input = Input(shape=(max_text_len, ))\n",
        "\n",
        "        # encoder embedding layer\n",
        "        encoder_embedding = Embedding(\n",
        "            x_vocab_size,\n",
        "            embedding_dim,\n",
        "            embeddings_initializer=tf.keras.initializers.Constant(x_embedding_matrix),\n",
        "            trainable=False,\n",
        "            name='encoder_embedding'\n",
        "        )(encoder_input)\n",
        "\n",
        "        # encoder lstm1\n",
        "        encoder_bi_lstm1 = Bidirectional(\n",
        "            LSTM(\n",
        "                latent_dim,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                dropout=0.4,\n",
        "                recurrent_dropout=0.4,\n",
        "                name='encoder_lstm_1'\n",
        "            ),\n",
        "            name='encoder_bidirectional_lstm_1'\n",
        "        )\n",
        "        encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_bi_lstm1(\n",
        "            encoder_embedding\n",
        "        )\n",
        "        encoder_bi_lstm1_output = [\n",
        "            encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1\n",
        "        ]\n",
        "\n",
        "        # encoder lstm 2\n",
        "        encoder_bi_lstm2 = Bidirectional(\n",
        "            LSTM(\n",
        "                latent_dim,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                dropout=0.4,\n",
        "                recurrent_dropout=0.4,\n",
        "                name='encoder_lstm_2'\n",
        "            ),\n",
        "            name='encoder_bidirectional_lstm_2'\n",
        "        )\n",
        "        encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_bi_lstm2(\n",
        "            encoder_output1\n",
        "        )\n",
        "        encoder_bi_lstm2_output = [\n",
        "            encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2\n",
        "        ]\n",
        "\n",
        "        # encoder lstm 3\n",
        "        encoder_bi_lstm = Bidirectional(\n",
        "            LSTM(\n",
        "                latent_dim,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                dropout=0.4,\n",
        "                recurrent_dropout=0.4,\n",
        "                name='encoder_lstm_3'\n",
        "            ),\n",
        "            name='encoder_bidirectional_lstm_3'\n",
        "        )\n",
        "        encoder_output, *encoder_final_states = encoder_bi_lstm(encoder_output2)\n",
        "\n",
        "        # =====================\n",
        "        #  Decoder\n",
        "        # =====================\n",
        "\n",
        "        # Set up the decoder, using `encoder_states` as initial state.\n",
        "\n",
        "        decoder_input = Input(shape=(None, ))\n",
        "\n",
        "        # decoder embedding layer\n",
        "        decoder_embedding_layer = Embedding(\n",
        "            y_vocab_size,\n",
        "            embedding_dim,\n",
        "            embeddings_initializer=tf.keras.initializers.Constant(y_embedding_matrix),\n",
        "            trainable=False,\n",
        "            name='decoder_embedding'\n",
        "        )\n",
        "        decoder_embedding = decoder_embedding_layer(decoder_input)\n",
        "        \n",
        "        decoder_lstm = LSTM(\n",
        "            latent_dim,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            dropout=0.4,\n",
        "            recurrent_dropout=0.2,\n",
        "            name='decoder_lstm_1'\n",
        "        )\n",
        "        decoder_output, *decoder_final_states = decoder_lstm(\n",
        "            decoder_embedding, initial_state=encoder_final_states[:2]\n",
        "        )  # taking only the forward states\n",
        "\n",
        "        # dense layer\n",
        "        decoder_dense = TimeDistributed(\n",
        "            Dense(y_vocab_size, activation='softmax')\n",
        "        )\n",
        "        decoder_output = decoder_dense(decoder_output)\n",
        "\n",
        "        # =====================\n",
        "        #  Model\n",
        "        # =====================\n",
        "        model = Model([encoder_input, decoder_input], decoder_output, name='seq2seq_model_with_bidirectional_lstm')\n",
        "        model.summary()\n",
        "\n",
        "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'model': model,\n",
        "            'inputs': {\n",
        "                'encoder': encoder_input,\n",
        "                'decoder': decoder_input\n",
        "            },\n",
        "            'outputs': {\n",
        "                'encoder': encoder_output,\n",
        "                'decoder': decoder_output\n",
        "            },\n",
        "            'states': {\n",
        "                'encoder': encoder_final_states,\n",
        "                'decoder': decoder_final_states\n",
        "            },\n",
        "            'layers': {\n",
        "                'decoder': {\n",
        "                    'embedding': decoder_embedding_layer,\n",
        "                    'last_decoder_lstm': decoder_lstm,\n",
        "                    'dense': decoder_dense\n",
        "                }\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "liquid-piece",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T01:59:59.528294Z",
          "iopub.status.busy": "2021-04-05T01:59:59.527284Z",
          "iopub.status.idle": "2021-04-05T02:00:03.897115Z",
          "shell.execute_reply": "2021-04-05T02:00:03.896515Z"
        },
        "papermill": {
          "duration": 4.504504,
          "end_time": "2021-04-05T02:00:03.897259",
          "exception": false,
          "start_time": "2021-04-05T01:59:59.392755",
          "status": "completed"
        },
        "tags": [],
        "id": "liquid-piece"
      },
      "outputs": [],
      "source": [
        "seq2seq = build_seq2seq_model_with_just_lstm(\n",
        "    embedding_dim, latent_dim, max_text_len, \n",
        "    x_vocab_size, y_vocab_size,\n",
        "    x_embedding_matrix, y_embedding_matrix\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "theoretical-feelings",
      "metadata": {
        "papermill": {
          "duration": 0.132236,
          "end_time": "2021-04-05T02:00:04.165673",
          "exception": false,
          "start_time": "2021-04-05T02:00:04.033437",
          "status": "completed"
        },
        "tags": [],
        "id": "theoretical-feelings"
      },
      "source": [
        "If you want to change `model` then just change the `function name` above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "quiet-bangkok",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T02:00:04.440081Z",
          "iopub.status.busy": "2021-04-05T02:00:04.439060Z",
          "iopub.status.idle": "2021-04-05T02:00:04.442523Z",
          "shell.execute_reply": "2021-04-05T02:00:04.441982Z"
        },
        "papermill": {
          "duration": 0.142976,
          "end_time": "2021-04-05T02:00:04.442674",
          "exception": false,
          "start_time": "2021-04-05T02:00:04.299698",
          "status": "completed"
        },
        "tags": [],
        "id": "quiet-bangkok"
      },
      "outputs": [],
      "source": [
        "model = seq2seq['model']\n",
        "\n",
        "encoder_input = seq2seq['inputs']['encoder']\n",
        "decoder_input = seq2seq['inputs']['decoder']\n",
        "\n",
        "encoder_output = seq2seq['outputs']['encoder']\n",
        "decoder_output = seq2seq['outputs']['decoder']\n",
        "\n",
        "encoder_final_states = seq2seq['states']['encoder']\n",
        "decoder_final_states = seq2seq['states']['decoder']\n",
        "\n",
        "decoder_embedding_layer = seq2seq['layers']['decoder']['embedding']\n",
        "last_decoder_lstm = seq2seq['layers']['decoder']['last_decoder_lstm']\n",
        "decoder_dense = seq2seq['layers']['decoder']['dense']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solved-promotion",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T02:00:04.716081Z",
          "iopub.status.busy": "2021-04-05T02:00:04.715001Z",
          "iopub.status.idle": "2021-04-05T02:00:04.719122Z",
          "shell.execute_reply": "2021-04-05T02:00:04.718451Z"
        },
        "papermill": {
          "duration": 0.142486,
          "end_time": "2021-04-05T02:00:04.719260",
          "exception": false,
          "start_time": "2021-04-05T02:00:04.576774",
          "status": "completed"
        },
        "tags": [],
        "id": "solved-promotion"
      },
      "outputs": [],
      "source": [
        "model.layers[-2].input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "upper-facing",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T02:00:04.993644Z",
          "iopub.status.busy": "2021-04-05T02:00:04.992686Z",
          "iopub.status.idle": "2021-04-05T02:00:04.995829Z",
          "shell.execute_reply": "2021-04-05T02:00:04.995188Z"
        },
        "papermill": {
          "duration": 0.142739,
          "end_time": "2021-04-05T02:00:04.995976",
          "exception": false,
          "start_time": "2021-04-05T02:00:04.853237",
          "status": "completed"
        },
        "tags": [],
        "id": "upper-facing"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.000001, verbose=1),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comic-austin",
      "metadata": {
        "papermill": {
          "duration": 0.134387,
          "end_time": "2021-04-05T02:00:05.264750",
          "exception": false,
          "start_time": "2021-04-05T02:00:05.130363",
          "status": "completed"
        },
        "tags": [],
        "id": "comic-austin"
      },
      "source": [
        "Use a `tuple` instead of `list` in `validation_parameter` in `model.fit()`, to know the reason reading this [post](https://stackoverflow.com/questions/61586981/valueerror-layer-sequential-20-expects-1-inputs-but-it-received-2-input-tensor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fifty-botswana",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T02:00:05.542301Z",
          "iopub.status.busy": "2021-04-05T02:00:05.541620Z",
          "iopub.status.idle": "2021-04-05T02:03:39.866710Z",
          "shell.execute_reply": "2021-04-05T02:03:39.867244Z"
        },
        "papermill": {
          "duration": 214.46664,
          "end_time": "2021-04-05T02:03:39.867609",
          "exception": false,
          "start_time": "2021-04-05T02:00:05.400969",
          "status": "completed"
        },
        "tags": [],
        "id": "fifty-botswana"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    [x_train_padded, y_train_padded[:, :-1]],\n",
        "    y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:, 1:],\n",
        "    epochs=num_epochs,\n",
        "    batch_size=128 * tpu_strategy.num_replicas_in_sync,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=(\n",
        "        [x_val_padded, y_val_padded[:, :-1]],\n",
        "        y_val_padded.reshape(y_val_padded.shape[0], y_val_padded.shape[1], 1)[:, 1:]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "developmental-collect",
      "metadata": {
        "papermill": {
          "duration": 0.681139,
          "end_time": "2021-04-05T02:03:41.228236",
          "exception": false,
          "start_time": "2021-04-05T02:03:40.547097",
          "status": "completed"
        },
        "tags": [],
        "id": "developmental-collect"
      },
      "source": [
        "**Plotting model's performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dimensional-production",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T02:03:42.598644Z",
          "iopub.status.busy": "2021-04-05T02:03:42.597357Z",
          "iopub.status.idle": "2021-04-05T02:03:42.771525Z",
          "shell.execute_reply": "2021-04-05T02:03:42.772080Z"
        },
        "papermill": {
          "duration": 0.865569,
          "end_time": "2021-04-05T02:03:42.772266",
          "exception": false,
          "start_time": "2021-04-05T02:03:41.906697",
          "status": "completed"
        },
        "tags": [],
        "id": "dimensional-production"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "plt.plot(history.history['accuracy'][1:], label='train acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "liable-silence",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-05T02:03:44.140312Z",
          "iopub.status.busy": "2021-04-05T02:03:44.139646Z",
          "iopub.status.idle": "2021-04-05T02:03:44.298827Z",
          "shell.execute_reply": "2021-04-05T02:03:44.299385Z"
        },
        "papermill": {
          "duration": 0.843348,
          "end_time": "2021-04-05T02:03:44.299565",
          "exception": false,
          "start_time": "2021-04-05T02:03:43.456217",
          "status": "completed"
        },
        "tags": [],
        "id": "liable-silence"
      },
      "outputs": [],
      "source": [
        "# Loss\n",
        "plt.plot(history.history['loss'][1:], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='lower right')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1835.082356,
      "end_time": "2021-04-05T02:24:54.120622",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-04-05T01:54:19.038266",
      "version": "2.3.2"
    },
    "colab": {
      "name": "Football_Commentary_Summarization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}